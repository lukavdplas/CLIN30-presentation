{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "from reach import Reach\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "import sklearn.preprocessing\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathnames\n",
    "relpron_path = './relpron_translation.txt'\n",
    "verbs_path = './verbmatrices'\n",
    "vectorspath = './embeddings/sonar-160.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the RELPRON test sentences. This dataset consists of pairs of nouns and relative clauses with a transtitive verb, describing some property or action related to them. An example:\n",
    "\n",
    "> SBJ vliegtuig: vaartuig dat hoogte bereikt/bereik\n",
    "> [SBJ airplaine: craft that reaches height]\n",
    "\n",
    "Note that items also list whether the clause is subject or object relative, and gives the root for inflected verbs (and nouns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relpron data\n",
    "relpron_file = open(relpron_path,'r', encoding='latin-1')\n",
    "items_raw = relpron_file.readlines()\n",
    "relpron_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store relpron data\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, string):\n",
    "        key, self.propertystr = string.strip().split(': ')\n",
    "        \n",
    "        self.relation, self.termN = key.split(' ')\n",
    "        self.rel = self.relation[0]\n",
    "        \n",
    "        propertylst = self.propertystr.split(' ')\n",
    "        self.headN = propertylst[0]\n",
    "        self.argN = propertylst[2].split('/')[0]\n",
    "        _, self.V = propertylst[-1].split('/')\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.termN + ': ' + self.propertystr\n",
    "        \n",
    "verbfiles = [filename[:-4] for filename in os.listdir(verbs_path)]\n",
    "\n",
    "items = dict()\n",
    "for item in items_raw:\n",
    "    sent = Sentence(item)\n",
    "    if sent.V + '|O' in verbfiles and sent.V + '|S' in verbfiles:\n",
    "        if sent.termN not in items:\n",
    "            items[sent.termN] = [sent]\n",
    "        else:\n",
    "            items[sent.termN].append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the noun embeddings and the verb matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load vectors\n",
    "r = Reach.load(vectorspath)\n",
    "\n",
    "#load matrices\n",
    "verbmatrices = dict()\n",
    "for verbfile in os.listdir(verbs_path):\n",
    "    verbmatrices[verbfile[:-4]] = np.load(verbs_path+'/'+verbfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate composed vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_control = True\n",
    "mean_std = 0.08\n",
    "\n",
    "class PhraseComposer:\n",
    "    def __init__(self, sentence):\n",
    "        \"\"\"Load vectors and matrices for the phrase\"\"\"\n",
    "        self.relation = sentence.rel\n",
    "        if self.relation == 'O':\n",
    "            self.inverse_relation = 'S'\n",
    "        else:\n",
    "            self.inverse_relation = 'O'\n",
    "        \n",
    "        try:\n",
    "            self.arg_v = r[sentence.argN]\n",
    "            self.arg_v = self.Scale(self.arg_v)\n",
    "            \n",
    "            self.head_v = r[sentence.headN]\n",
    "            self.head_v = self.Scale(self.head_v)\n",
    "            \n",
    "            self.V_v = r[sentence.V.split('_')[0]]\n",
    "            self.VO_m =  verbmatrices[sent.V+'|O']\n",
    "            self.VS_m = verbmatrices[sent.V+'|S']\n",
    "            \n",
    "            self.valid = True\n",
    "        \n",
    "        except KeyError:\n",
    "            self.valid = False\n",
    "        \n",
    "    def Scale(self, v):\n",
    "        \"\"\"Apply standard scaling to argument vector as was done in training.\"\"\"\n",
    "        scaler = sklearn.preprocessing.StandardScaler()\n",
    "        scaler.set_params(with_std=variance_control)\n",
    "        v = scaler.fit_transform(v[:, np.newaxis])\n",
    "        v = np.squeeze(v)\n",
    "        if variance_control:\n",
    "            v = mean_std * v\n",
    "        return v\n",
    "    \n",
    "    def Compose(self, method):\n",
    "        \"\"\"Direct to proper composition method based on input string\"\"\"\n",
    "        if method == 'Addition':\n",
    "            return self.Addition()\n",
    "        if method == 'PLF':\n",
    "            return self.PLF(self.relation)\n",
    "        if method == 'VArg':\n",
    "            return self.VArg(self.relation)\n",
    "        if method == 'VHn':\n",
    "            return self.VHn(self.relation)\n",
    "        if method == 'iPLF':\n",
    "            return self.PLF(self.inverse_relation)\n",
    "        \n",
    "    def Addition(self):\n",
    "        return np.add(self.head_v, self.arg_v, self.V_v)\n",
    "\n",
    "    def VHn(self, relation):\n",
    "        if not relation:\n",
    "            relation = self.relation  #cant put a property of the object as a default argument so keep it like this\n",
    "        if relation == 'O':\n",
    "            return np.dot(self.head_v, self.VO_m)\n",
    "        else:\n",
    "            return np.dot(self.head_v, self.VS_m)\n",
    "    \n",
    "    def VArg(self, relation):\n",
    "        if relation == 'O':\n",
    "            return np.dot(self.arg_v, self.VS_m)\n",
    "        else:\n",
    "            return np.dot(self.arg_v, self.VO_m)\n",
    "    \n",
    "    def PLF(self, relation):\n",
    "        return np.add(self.VArg(relation), self.VHn(relation))\n",
    "    \n",
    "    def iPLF(self):\n",
    "        return(np.add(self.VArg(self.inverse_relation), self.VHn(self.inverse_relation)))         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede786f0a8674262affe745519d23331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=111), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "methods = ['Addition', 'VHn', 'VArg', 'PLF', 'iPLF']\n",
    "methods_S = ['VHn', 'VArg', 'PLF', 'iPLF'] #methods with their result in the S space\n",
    "methods_N = ['Addition'] #methods with the results in the N space\n",
    "\n",
    "terms = items.keys()\n",
    "\n",
    "results = dict()\n",
    "\n",
    "for term in tqdm(terms):\n",
    "    sents = items[term]\n",
    "    for sent in sents:\n",
    "        composer = PhraseComposer(sent)\n",
    "        if composer.valid:\n",
    "            sentresults = dict()\n",
    "            for method in methods:\n",
    "                property_v = composer.Compose(method)\n",
    "                sentresults[method] = property_v\n",
    "\n",
    "            sentresults['target'] = r[sent.termN]\n",
    "            results[sent] = sentresults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check some sizes\n",
    "\n",
    "More of a technicality. \n",
    "\n",
    "Checking the number of trained verbs is a bit cumbersome, because subject and object transformations are trained separately, and were only included if they had enough data. So some verbs have a subject but no object representation, or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terms in corpus: 111\n",
      "Total number of relative clauses: 411\n",
      "Relative clauses discarded: 17\n",
      "Relative clauses included in results: 394\n",
      "Average number of clauses per term: 3.55\n",
      "Number of verbs: 122\n"
     ]
    }
   ],
   "source": [
    "print('Number of terms in corpus:', len(items))\n",
    "\n",
    "total_clauses = sum(len(items[term]) for term in items)\n",
    "print('Total number of relative clauses:', total_clauses)\n",
    "\n",
    "print('Relative clauses discarded:', total_clauses - len(results))\n",
    "\n",
    "print('Relative clauses included in results:', len(results))\n",
    "\n",
    "print('Average number of clauses per term:', round(len(results) / len(items), 2))\n",
    "\n",
    "roles_for_verb = lambda key : set(verb[-1] for verb in verbmatrices if verb[:-2] == key[:-2])\n",
    "verbs = {verbkey[:-2] : roles_for_verb(verbkey) for verbkey in verbmatrices}\n",
    "print('Number of verbs:', len([verb for verb in verbs if len(verbs[verb]) == 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate methods by ranking sentences based on term similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity functions\n",
    "\n",
    "def cosine(v1, v2):\n",
    "    product = np.dot(v1, v2)\n",
    "    norm = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
    "    return product / norm\n",
    "\n",
    "def correlation(v1, v2):\n",
    "    return stats.pearsonr(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank sentences based on similarity\n",
    "\n",
    "def rankSentences (target_v, results_dict, method='Addition', sim_function=cosine):\n",
    "    \"\"\"Returns a list of the results in the results dictionary, ranked by their similarity \n",
    "    with the target vector. Method and similarity function are parameters.\"\"\"\n",
    "\n",
    "    allsents = list(results_dict.keys())\n",
    "    similarity = lambda sent: sim_function(target_v, results_dict[sent][method])\n",
    "    return(sorted(allsents, key=similarity, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ranking(target_term, sorted_list):\n",
    "    \"\"\"Rate the ranking (output of rankSentences) for a sorted list using MAP. \n",
    "    The ideal ranking puts all sentences with the target term on the top. \n",
    "    Returns 1 for perfect sorting, 0 for worst sorting.\"\"\"\n",
    "    \n",
    "    #count how many sentences in the list use the target term\n",
    "    target_sents = list(filter(lambda sent: sent.termN == target_term, sorted_list))\n",
    "    count = len(target_sents)\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    for i in range(len(sorted_list)):\n",
    "        term = sorted_list[i].termN\n",
    "        if i < count:\n",
    "            correct = term == target_term\n",
    "            error = int(not correct) * (count - i) / count\n",
    "        else:\n",
    "            correct = term != target_term\n",
    "            error = int(not correct) * (i + 1 - count) / (count)\n",
    "        errors.append(error)\n",
    "    \n",
    "    return 1 - (sum(errors) / len(sorted_list))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the rankings to evaluate composition methods. If we evaluate a representation of a sentence, we rank all sentences in the corpus based on their similarity to the sentence vector. Ideally, all items with the same term are ranked on top.\n",
    "\n",
    "To make things a bit more spicy, some the functions below allow you to pass both a base method and a \"corpus method\". The *base method* is used to represent the sentence, while the *corpus method* is used to represent the rest of the corpus. This way we can ask all of the following questions:\n",
    "* Does PLF represent similar relative clauses close to each other?\n",
    "* Does inverted PLF still represent similar clauses close to each other?\n",
    "* If you invert the argument structure of a single sentence, does it still rank similarly to other, non-inverted sentences describing the same term?\n",
    "Note that cross-comparing methods like this is only possible if they represent clauses in the same vector space. All methods represent clauses in the S space except for addition and elementwise multiplication, which represent it in the N space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sent(sent, method, corpus_method, sim_function=cosine):\n",
    "    \"\"\"Evaluate the ranking based on the representation of a given sentence. It should rank sentence with \n",
    "    the same term on top.\"\"\"\n",
    "    term = sent.termN\n",
    "    target_v = results[sent][method]\n",
    "    ranking =  rankSentences(target_v, results, corpus_method, sim_function)\n",
    "    evaluation = evaluate_ranking(term, ranking)\n",
    "    return evaluation\n",
    "\n",
    "def evaluate_method(sentences, method, corpus_method, sim_function=cosine):\n",
    "    \"\"\"Evaluate a method by looping through all sentences.\"\"\"\n",
    "    if not corpus_method:\n",
    "        corpus_method = method\n",
    "        \n",
    "    evaluations = [evaluate_sent(sent, method, corpus_method, sim_function) for sent in sentences]\n",
    "    return round(sum(evaluations) / len(sentences), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do the methods compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tranking\n",
      "        Addition\t0.744\n",
      "             VHn\t0.819\n",
      "            VArg\t0.779\n",
      "             PLF\t0.802\n",
      "            iPLF\t0.798\n"
     ]
    }
   ],
   "source": [
    "print('\\t\\t\\tranking')\n",
    "\n",
    "for method in methods:\n",
    "    print(' ' * (16 - len(method)) + method, \n",
    "          round(evaluate_method(results.keys(), method), 3),\n",
    "          sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just of a bit of fun:\n",
    "#evaluate based on similarity between property vectors and term vectors (only useful for addition)\n",
    "\n",
    "terms = {sent.termN : results[sent]['target'] for sent in results.keys()}\n",
    "\n",
    "def evaluate_term(term, target_v, method='Addition', sim_function=cosine):\n",
    "    ranking = rankSentences(target_v, results, method, sim_function)\n",
    "    evaluation = evaluate_ranking(term, ranking)\n",
    "    return evaluation\n",
    "\n",
    "def evaluate_method_by_term(terms, method='Addition', sim_function=cosine):\n",
    "    evaluations = [evaluate_term(term, terms[term], method, sim_function) for term in terms]\n",
    "    return round(sum(evaluations) / len(terms), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity to PLF\n",
    "\n",
    "In the last section, we looked at composition methods in isolation. We notice that most of them represent semantic distance pretty well! But does that mean that their results are similar to the PLF results? Or are they preserving distance in their own space?\n",
    "\n",
    "One way to see this: how do they rank PLF results? PLF will give the same results as above (hopefully!). The other methods essentially represent some loss of information: we only consider the head noun, we only consider the argument, or we mess up the argument structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tranking\n",
      "             VHn\t0.8\n",
      "            VArg\t0.783\n",
      "             PLF\t0.802\n",
      "            iPLF\t0.797\n"
     ]
    }
   ],
   "source": [
    "print('\\t\\t\\tranking')\n",
    "\n",
    "for method in methods_S:\n",
    "    print(' ' * (16 - len(method)) + method, \n",
    "          round(evaluate_method(results.keys(), method = method, corpus_method = 'PLF'), 3),\n",
    "          sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers are still really high! Let's look at the direct similarity with the PLF results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_methods(method_1, method_2, sim_function = cosine):\n",
    "    similarities = [sim_function(results[sent][method_1], results[sent][method_2]) for sent in results]\n",
    "    return similarities\n",
    "\n",
    "methods_compared_to_PLF = {method : compare_methods(method, 'PLF') for method in methods_S}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VHn 0.9582530229166685\n",
      "VArg 0.949044962419609\n",
      "PLF 1.0\n",
      "iPLF 0.9682177219853254\n"
     ]
    }
   ],
   "source": [
    "for method in methods_compared_to_PLF:\n",
    "    print(method, sum(methods_compared_to_PLF[method])/len(methods_compared_to_PLF[method]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
